{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de ce TP est de se familiariser avec python et Scikit-learn afin de d√©couvrir certains mod√®les et algorithmes de science des donn√©es. Le TP contient aussi un certain nombre d'explications succintes de diff√©rents concepts importants qui seront revus plus en d√©tails dans le cours. Nous vous encourageons √† manipuler autant que possible les extraits de code fournis pour comprendre ce qu'ils recouvrent.\n",
    "\n",
    "Vous devez r√©pondre aux questions en rouge en cr√©ant de nouvelles cellules, texte ou code, contenant vos r√©ponses.\n",
    "\n",
    "Le TP est extrait du livre \"Python Data Science Handbook\" de Jake VanderPlas. \n",
    "\n",
    "Vous pouvez consulter un tutoriel python sous https://docs.python.org/fr/3/tutorial/\n",
    "\n",
    "\n",
    "\n",
    "# Using sklearn for building ML models\n",
    "\n",
    "\n",
    "The Scikit-Learn API is designed with the following guiding principles in mind, as outlined in the Scikit-Learn API paper:\n",
    "\n",
    "* Consistency: All objects share a common interface drawn from a limited set of methods, with consistent documentation.\n",
    "\n",
    "* Inspection: All specified parameter values are exposed as public attributes.\n",
    "\n",
    "* Limited object hierarchy: Only algorithms are represented by Python classes; datasets are represented in standard formats (NumPy arrays, Pandas DataFrames, SciPy sparse matrices) and parameter names use standard Python strings.\n",
    "\n",
    "* Composition: Many machine learning tasks can be expressed as sequences of more fundamental algorithms, and Scikit-Learn makes use of this wherever possible.\n",
    "\n",
    "* Sensible defaults: When models require user-specified parameters, the library defines an appropriate default value.\n",
    "\n",
    "In practice, these principles make Scikit-Learn very easy to use, once the basic principles are understood. Every machine learning algorithm in Scikit-Learn is implemented via the Estimator API, which provides a consistent interface for a wide range of machine learning applications.\n",
    "\n",
    "## Basics of the API\n",
    "\n",
    "Most commonly, the steps in using the Scikit-Learn estimator API are as follows (we will step through a handful of detailed examples in the sections that follow).\n",
    "\n",
    "* Choose a class of model by importing the appropriate estimator class from Scikit-Learn.\n",
    "* Choose model hyperparameters by instantiating this class with desired values.\n",
    "* Arrange data into a features matrix and target vector following the discussion above.\n",
    "* Fit the model to your data by calling the fit() method of the model instance.\n",
    "* Apply the Model to new data:\n",
    "  * For supervised learning, often we predict labels for unknown data using the predict() method.\n",
    "  * For unsupervised learning, we often transform or infer properties of the data using the transform() or predict() method.\n",
    "\n",
    "We will now step through several simple examples of applying supervised and unsupervised learning methods.\n",
    "\n",
    "## Supervised learning example: Simple linear regression\n",
    "\n",
    "As an example of this process, let's consider a simple linear regression‚Äîthat is, the common case of fitting a line to  (ùë•,ùë¶)  data. We will use the following simple data for our regression example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 1 + rng.randn(50)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Quel est le type de x ? de y ? Afficher x et y</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now walk through the process of building an ML model\n",
    "\n",
    "1. Choose a class of model\n",
    "\n",
    "In Scikit-Learn, every class of model is represented by a Python class. So, for example, if we would like to compute a simple linear regression model, we can import the linear regression class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Choose model hyperparameters\n",
    "\n",
    "An important point is that a class of model is not the same as an instance of a model.\n",
    "\n",
    "Once we have decided on our model class, there are still some options open to us. Depending on the model class we are working with, we might need to answer one or more questions like the following:\n",
    "\n",
    "Would we like to fit for the offset (i.e., y-intercept)?\n",
    "Would we like the model to be normalized?\n",
    "Would we like to preprocess our features to add model flexibility?\n",
    "What degree of regularization would we like to use in our model?\n",
    "How many model components would we like to use?\n",
    "These are examples of the important choices that must be made once the model class is selected. These choices are often represented as hyperparameters, or parameters that must be set before the model is fit to data. In Scikit-Learn, hyperparameters are chosen by passing values at model instantiation. We will explore how you can quantitatively motivate the choice of hyperparameters later.\n",
    "\n",
    "For our linear regression example, we can instantiate the LinearRegression class and specify that we would like to fit the intercept using the fit_intercept hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that when the model is instantiated, the only action is the storing of these hyperparameter values. In particular, we have not yet applied the model to any data: the Scikit-Learn API makes very clear the distinction between choice of model and application of model to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Arrange data into a features matrix and target vector\n",
    "\n",
    "Previously we detailed the Scikit-Learn data representation, which requires a two-dimensional features matrix and a one-dimensional target array. Here our target variable y is already in the correct form (a length-n_samples array), but we need to massage the data x to make it a matrix of size [n_samples, n_features]. In this case, this amounts to a simple reshaping of the one-dimensional array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x[:, np.newaxis]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fit the model to your data\n",
    "\n",
    "Now it is time to apply our model to data. This can be done with the fit() method of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fit() command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in model-specific attributes that the user can explore. In Scikit-Learn, by convention all model parameters that were learned during the fit() process have trailing underscores; for example in this linear model, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9033107255311164"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two parameters represent the slope and intercept of the simple linear fit to the data. Comparing to the data definition, we see that they are very close to the input slope of 2 and intercept of -1.\n",
    "\n",
    "5. Predict labels for unknown data\n",
    "\n",
    "Once the model is trained, the main task of supervised machine learning is to evaluate it based on what it says about new data that was not part of the training set. In Scikit-Learn, this can be done using the predict() method. For the sake of this example, our \"new data\" will be a grid of x values, and we will ask what y values the model predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Quel est l'effet de linspace ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to coerce these x values into a [n_samples, n_features] features matrix, after which we can feed it to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfit = xfit[:, np.newaxis]\n",
    "yfit = model.predict(Xfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Quel est l'effet de xfit ? de model.predict ? quel est le type de Xfit ? de yfit ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize the results by plotting first the raw data, and then this model fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised classification: Naive bayes on Iris dataset\n",
    "\n",
    "Let's take a look at another example of this process, using the Iris dataset we discussed earlier. Our question will be this: given a model trained on a portion of the Iris data, how well can we predict the remaining labels?\n",
    "\n",
    "For this task, we will use an extremely simple generative model known as Gaussian naive Bayes, which proceeds by assuming each class is drawn from an axis-aligned Gaussian distribution. Because it is so fast and has no hyperparameters to choose, Gaussian naive Bayes is often a good model to use as a baseline classification, before exploring whether improvements can be found through more sophisticated models.\n",
    "\n",
    "We would like to evaluate the model on data it has not seen before, and so we will split the data into a training set and a testing set. This could be done by hand, but it is more convenient to use the train_test_split utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "dataset = load_iris()\n",
    "X_iris = dataset.data\n",
    "y_iris = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris,\n",
    "                                                random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Quelle est l'utilit√© de random_state ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data arranged, we can follow our recipe to predict the labels.\n",
    "\n",
    "<span style=\"color:red\">En utilisant le code suivant, apprendre un mod√®le Naive Bayes (cf. GaussianNB) sur les donn√©es Iris et stocker les pr√©dictions faites sur Xtest dans un vecteur que vous appellerez y_model:</span>\n",
    "\n",
    "<span style=\"color:green\">from sklearn.naive_bayes import GaussianNB # 1. choose model class</span>\n",
    "\n",
    "<span style=\"color:green\">model = GaussianNB()                       # 2. instantiate model</span>\n",
    "\n",
    "<span style=\"color:green\">model.fit(Xtrain, ytrain)                  # 3. fit model to data</span>\n",
    "\n",
    "<span style=\"color:green\">y_model = model.predict(Xtest)             # 4. predict on new data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the accuracy_score utility to see the fraction of predicted labels that match their true value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-414695b0788c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">√Ä quoi correspond l'accuracy ? Donner une formule et une explication.</span>\n",
    "\n",
    "<span style=\"color:red\">Quelle est la matrice de confusion du mod√®le Naive Bayes sur les donn√©es Iris ? Ecrire le script permettant le calcul et la visualisation de cette matrice.</span>\n",
    "\n",
    "<span style=\"color:red\">Reprendre les √©tapes ci-dessus avec un mod√®le k-NN (toujours sur les donn√©es Iris).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning: Iris clustering\n",
    "\n",
    "Let's next look at applying clustering to the Iris data. A clustering algorithm attempts to find distinct groups of data without reference to any labels. Here we will use the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Charger le jeu de donn√©es iris (voir TP k-moyennes/k-means) et utiliser le code k-means de sklearn. Visualiser les r√©sultats (voir TP k-moyennes/k-means).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sklearn to validate ML models and hyperparameters \n",
    "\n",
    "In the previous section, we saw the basic recipe for applying a supervised machine learning model:\n",
    "\n",
    "1. Choose a class of model\n",
    "2. Choose model hyperparameters\n",
    "3. Fit the model to the training data\n",
    "4. Use the model to predict labels for new data\n",
    "\n",
    "The first two pieces of this‚Äîthe choice of model and choice of hyperparameters‚Äîare perhaps the most important part of using these tools and techniques effectively. In order to make an informed choice, we need a way to validate that our model and our hyperparameters are a good fit to the data. While this may sound simple, there are some pitfalls that you must avoid to do this effectively.\n",
    "\n",
    "## Thinking about Model Validation\n",
    "\n",
    "In principle, model validation is very simple: after choosing a model and its hyperparameters, we can estimate how effective it is by applying it to some of the training data and comparing the prediction to the known value.\n",
    "\n",
    "The following sections first show a naive approach to model validation and why it fails, before exploring the use of holdout sets and cross-validation for more robust model evaluation.\n",
    "\n",
    "### Model validation the wrong way\n",
    "\n",
    "Let's demonstrate the naive approach to validation using the Iris data, which we saw in the previous section. We will start by loading the data:\n",
    "\n",
    "<span style=\"color:red\">Charger le dataset Iris avec les attributs dans un tableau X et les targets dans un vecteur y.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we choose a model and hyperparameters. Here we'll use a k-neighbors classifier with n_neighbors=1. This is a very simple and intuitive model that says \"the label of an unknown point is the same as the label of its closest training point:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the model, and use it to predict labels for data we already know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "y_model = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the fraction of correctly labeled points:\n",
    "\n",
    "<span style=\"color:red\">Calculer la pr√©cision (accuracy) de la prediction y_model. Quel r√©sultat observe-t-on et pourquoi ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation the right way: Holdout sets\n",
    "\n",
    "So what can be done? A better sense of a model's performance can be found using what's known as a holdout set: that is, we hold back some subset of the data from the training of the model, and then use this holdout set to check the model performance. This splitting can be done using the train_test_split utility in Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split the data with 50% in each set\n",
    "X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n",
    "                                  train_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Fitter le mod√®le sur le premier sous-dataset X1, y1.</span>\n",
    "\n",
    "<span style=\"color:red\">Pr√©dire les classes sur le 2√®me sous-dataset X2, y2 et calculer la pr√©cision (accuracy). Qu'oberve-t-on ? Quel est l'effet de random_state ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model validation via cross-validation\n",
    "One disadvantage of using a holdout set for model validation is that we have lost a portion of our data to the model training. In the above case, half the dataset does not contribute to the training of the model! This is not optimal, and can cause problems ‚Äì especially if the initial set of training data is small.\n",
    "\n",
    "One way to address this is to use cross-validation; that is, to do a sequence of fits where each subset of the data is used both as a training set and as a validation set. For instance we can two validation trials, alternately using each half of the data as a holdout set. Using the split data from before, we could implement it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y2_model = model.fit(X1, y1).predict(X2)\n",
    "y1_model = model.fit(X2, y2).predict(X1)\n",
    "accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What comes out are two accuracy scores, which we could combine (by, say, taking the mean) to get a better measure of the global model performance. This particular form of cross-validation is a two-fold cross-validation‚Äîthat is, one in which we have split the data into two sets and used each in turn as a validation set.\n",
    "\n",
    "We could expand on this idea to use even more trials, and more folds in the data‚Äîfor example we can split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data. This would be rather tedious to do by hand, and so we can use Scikit-Learn's cross_val_score convenience routine to do it succinctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the validation across different subsets of the data gives us an even better idea of the performance of the algorithm.\n",
    "\n",
    "Scikit-Learn implements a number of useful cross-validation schemes that are useful in particular situations; these are implemented via iterators in the cross_validation module. For example, we might wish to go to the extreme case in which our number of folds is equal to the number of data points: that is, we train on all points but one in each trial. This type of cross-validation is known as leave-one-out cross validation, and can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "scores = cross_val_score(model, X, y, cv=LeaveOneOut())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Quel est la forme du r√©sultat scores et pourquoi ? Calculer la moyenne.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Best Model\n",
    "\n",
    "Now that we've seen the basics of validation and cross-validation, we will go into a litte more depth regarding model selection and selection of hyperparameters. These issues are some of the most important aspects of the practice of machine learning.\n",
    "\n",
    "Of core importance is the following question: if our estimator is underperforming, how should we move forward? There are several possible answers:\n",
    "\n",
    "* Use a more complicated/more flexible model\n",
    "* Use a less complicated/less flexible model\n",
    "* Gather more training samples\n",
    "* Gather more data to add features to each sample\n",
    "\n",
    "The answer to this question is often counter-intuitive. In particular, sometimes using a more complicated model will give worse results, and adding more training samples may not improve your results! The ability to determine what steps will improve your model is what separates the successful machine learning practitioners from the unsuccessful.\n",
    "\n",
    "### Validation curve\n",
    "\n",
    "Dans la suite, nous pr√©sentons une fa√ßon d'√©valuer la meilleure complixit√© de mod√®le √† choisir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we imagine that we have some ability to tune the model complexity, we would expect the training score and validation score to behave as illustrated in [this figure](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.03-validation-curve.png)\n",
    "\n",
    "The diagram shown here is often called a *validation curve*, and we see the following essential features:\n",
    "\n",
    "- The training score is everywhere higher than the validation score. This is generally the case: the model will be a better fit to data it has seen than to data it has not seen.\n",
    "- For very low model complexity (a high-bias model), the training data is under-fit, which means that the model is a poor predictor both for the training data and for any previously unseen data.\n",
    "- For very high model complexity (a high-variance model), the training data is over-fit, which means that the model predicts the training data very well, but fails for any previously unseen data.\n",
    "- For some intermediate value, the validation curve has a maximum. This level of complexity indicates a suitable trade-off between bias and variance.\n",
    "\n",
    "The means of tuning the model complexity varies from model to model; when we discuss individual models in depth in later sections, we will see how each model allows for such tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation curves in Scikit-Learn\n",
    "\n",
    "Let's look at an example of using cross-validation to compute the validation curve for a class of models.\n",
    "Here we will use a *polynomial regression* model: this is a generalized linear model in which the degree of the polynomial is a tunable parameter.\n",
    "For example, a degree-1 polynomial fits a straight line to the data; for model parameters $a$ and $b$:\n",
    "\n",
    "$$\n",
    "y = ax + b\n",
    "$$\n",
    "\n",
    "A degree-3 polynomial fits a cubic curve to the data; for model parameters $a, b, c, d$:\n",
    "\n",
    "$$\n",
    "y = ax^3 + bx^2 + cx + d\n",
    "$$\n",
    "\n",
    "We can generalize this to any number of polynomial features.\n",
    "In Scikit-Learn, we can implement this with a simple linear regression combined with the polynomial preprocessor.\n",
    "We will use a *pipeline* to string these operations together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Quel est l'effet de PolynomialRegression ? Quel est l'effet de kwargs ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some data to which we will fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_data(N, err=1.0, rseed=1):\n",
    "    # randomly sample the data\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(N, 1) ** 2\n",
    "    y = 10 - 1. / (X.ravel() + 0.1)\n",
    "    if err > 0:\n",
    "        y += err * rng.randn(N)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_data(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize our data, along with polynomial fits of several degrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "plt.scatter(X.ravel(), y, color='black')\n",
    "axis = plt.axis()\n",
    "for degree in [1, 3, 5]:\n",
    "    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n",
    "    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\n",
    "plt.xlim(-0.1, 1.0)\n",
    "plt.ylim(-2, 12)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Tracer des mod√®les avec des degr√©s de polynomes croissants. Qu'observe-t-on et pourquoi ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knob controlling model complexity in this case is the degree of the polynomial, which can be any non-negative integer. A useful question to answer is this: what is a good degree of polynomial?\n",
    "\n",
    "We can make progress in this by visualizing the validation curve for this particular data and model; this can be done straightforwardly using the validation_curve convenience routine provided by Scikit-Learn. Given a model, data, parameter name, and a range to explore, this function will automatically compute both the training score and validation score across the range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "degree = np.arange(0, 21)\n",
    "train_score, val_score = validation_curve(PolynomialRegression(), X, y,\n",
    "                                          'polynomialfeatures__degree', degree, cv=7)\n",
    "\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Retrouve-t-on le r√©sultat qualitatif attendu ?</span>\n",
    "\n",
    "<span style=\"color:red\">Quel est le meilleur degr√© de polyn√¥me √† utiliser ? Tracer le dataset avec le mod√®le de degr√© correspondant.</span>\n",
    "\n",
    "<span style=\"color:red\">Recommencez l'√©tude en modifiant la taille du dataset. Comment varient les r√©sultat ?</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
